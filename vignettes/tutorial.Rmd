---
title: "Project 3: STAT302package Tutorial"
author: "Judy Tian"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{STAT302package Tutorial}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Introduction

This is a R package including four functions: T-test function `my_t.test`, Linear Model function `my_lm`, Random Forest Cross-Validation function `my_knn_cv` and K-Nearest Neighbors Cross-Validation function `my_rf_cv`.

You can install the package through GitHub using:

``` {r, eval = FALSE}
devtools::install_github("JudyTi/STAT302package")
```

To begin, please load the package:

```{r setup}
library(STAT302package)
library(ggplot2)
data("my_gapminder")
data("my_penguins")
```

To view the vignette:

``` {r, eval = FALSE}
devtools::install_github("JudyTi/STAT302package", build_vignette = TRUE, build_opts = c())
library(STAT302package)
# Use this to view the vignette in the STAT302package HTML help
help(package = "STAT302package", help_type = "html")
# Use this to view the vignette as an isolated HTML file
utils::browseVignettes(package = "STAT302package")
```

## Tutorial for `my_t.test`

This function performs a one sample t-test in R. 

To demonstrate and present the function, the following three tests will use the `lifeExp` data from `my_gapminder` with the p-value cut-off of $\alpha = 0.05$.

### Two side test

The first hypothesis test is:
\[\begin{align}
  H_0: \mu &= 60,\\
  H_a: \mu &\neq 60.
  \end{align}\]
  
This is saying that the default null hypothesis $H_0$ is `mu` which equals to 60 and the alternative hypothesis $H_a$ is `mu` which isn't equal to 60. It's an example of two side test.

For this two side test, we could use the following code:

``` {r}
# two-sided t-test
two_side <- my_t.test(my_gapminder$lifeExp, alternative = "two.sided", mu = 60)
```
  
Thus the p value of this test is `r two_side$p_val`. This is greater than our pre-determined cut-off of 0.05, so we conclude that the result isn't statistically significant. Therefore, we don't reject null hypothesis $H_0$.

### One side test -- Lower tail

The second hypothesis test is:
\[\begin{align}
  H_0: \mu &= 60,\\
  H_a: \mu &< 60.
  \end{align}\]

This is saying that the default null hypothesis $H_0$ is `mu` which equals to 60 and the alternative hypothesis $H_a$ is `mu` which is less than 60. It's an example of two side test.

For this one side lower tail test, we could use the following code:

``` {r}
# one-sided t-test -- lower tail
lower_test <- my_t.test(my_gapminder$lifeExp, alternative = "less", mu = 60)
```

Thus the p value of this test is `r lower_test$p_val`. This is less than our pre-determined cut-off of 0.05, so we conclude that the result is statistically significant. Therefore, we reject null hypothesis $H_0$.

### One side test -- Upper tail

The third hypothesis test is:
\[\begin{align}
  H_0: \mu &= 60,\\
  H_a: \mu &> 60.
  \end{align}\]

This is saying that the default null hypothesis $H_0$ is `mu` which equals to 60 and the alternative hypothesis $H_a$ is `mu` which is greater to 60. It's an example of two side test.

For this one side upper tail test, we could use the following code:

``` {r}
# one-sided t-test -- upper tail
upper_test <- my_t.test(my_gapminder$lifeExp, alternative = "greater", mu = 60)
```

Thus the p value of this test is `r upper_test$p_val`. This is greater than our pre-determined cut-off of 0.05, so we conclude that the result isn't statistically significant. Therefore, we don't reject null hypothesis $H_0$.

## Tutorial for `my_lm`

To demonstrate a regression, we use `lifeExp` as response variable and `gdpPercap` and `continent` as explanatory variables using the following code:

``` {r, warning=FALSE}
# Demonstrate a regression
(lm_tut <- my_lm(lifeExp ~ gdpPercap + continent, data = my_gapminder))
```

From the result of `gdpPercap`, we could get the expected difference `r lm_tut[2]` in the response between two observations differing by one unit of `gdpPercap`, with all other covariates identical.

Then the hypothesis test associated with the `gdpPercap` coefficient is: 
\[\begin{align}
  H_0: \mu &= 0,\\
  H_a: \mu &\neq 0.
  \end{align}\]

with the p-value cut-off of $\alpha = 0.05$.

From the value of `Pr(>|t|)`, as it is less than our pre-determined cut-off of 0.05, so we conclude that the result is statistically significant. Therefore, we reject null hypothesis $H_0$. 

Next, we can use the following code to plot the Actual vs. Fitted values:

``` {r, fig.width=6.8, fig.height=4.5}
# Create data frame
x <- model.matrix(lifeExp ~ gdpPercap + continent, data = my_gapminder)
mod_fits <- x %*% lm_tut[, 1]
my_df <- data.frame("fitted" = mod_fits, 
                    "actual" = my_gapminder$lifeExp, 
                    "continent" = my_gapminder$continent)
# Plot Actual vs. Fitted for the response
ggplot(my_df, aes(x = fitted, y = actual, color = continent)) +
  geom_point(size = 0.1) +
  geom_abline(slope = 1, intercept = 0, col = "black", lty = 2) + 
  labs(x = "Fitted values", y = "Actual values", title = "Actual vs. Fitted") +
  theme(plot.title = element_text(hjust = 0.5))
```

From the graph, we could see the trends of Europe and Oceania are fit with the line whereas the others aren't. This implies the model we used is good for Europe and Oceania, but for other continent we probably need other models.

## Tutorial for `my_knn_cv` using `my_penguins`

Predict output class `species` using covariates `bill_length_mm`, `bill_depth_mm`, `flipper_length_mm`, and `body_mass_g` use 5-fold cross validation (`k_cv = 5`).

``` {r}
data <- na.omit(my_penguins)
train <- data[, 3:6]
train_miscl <- rep(NA, 10)
cv_miscl <- rep(NA, 10)
# Iterate from k_nn=1:10
for (i in (1:10)) {
  knn_result <- my_knn_cv(train, data$species, i, 5)
  # Calculate training misclassification rates
  train_miscl[i] <- mean(as.numeric(knn_result$class != data$species))
  # Get CV misclassification rates
  cv_miscl[i] <- knn_result$cv_err
}
my_knn_result <- data.frame("k_nn" = 1:10, 
                            "train_miscl" = train_miscl, 
                            "cv_miscl" = cv_miscl)
# Build table to display the values.
kableExtra::kable_styling(knitr::kable(my_knn_result))
```

Based on training misclassification rates, I would choose `k_nn = 1` because the value is the smallest which is 0.
And based on CV misclassification rates, I still would choose `k_nn = 1` because the value is again the smallest.

Therefore, in practice I would choose `k_nn = 1` because both training and cross-validation has the minimum test error at `k_nn = 1`.

## Tutorial for `my_rf_cv`

``` {r, fig.width=6.8, fig.height=6}
cv_num <- matrix(NA, 30, 3)
sd_cv <- rep(NA, 3)
a <- 1

# Iterate through k in (2, 5, 10)
for (i in c(2, 5, 10)) {
  # For each value of k, run function 30 times
  for (j in 1:30) {
    # Call my function tp get CV estimated MSE
    cv_num[j, a] <- my_rf_cv(i)
  }
  sd_cv[a] <- sd(cv_num[, a])
  a <- a + 1
}

# Build data frame of output
out1 <- data.frame("k" = "2", "result" = cv_num[, 1])
out2 <- data.frame("k" = "5", "result" = cv_num[, 2])
out3 <- data.frame("k" = "10", "result" = cv_num[, 3])
cv_result <- rbind(out1, out2, out3)
# Plot 3 boxplots to display data
ggplot(data = cv_result, aes(x = factor(k), y = result, fill = k)) + 
  geom_boxplot() +
  labs(x = "k",
       y = "CV MSE",
       title = "3 boxplots of CV MSE from 30 simulations") +
  theme(plot.title = element_text(hjust = 0.5))

# Build table to display the values.
cv_table <- data.frame("mean" = colMeans(cv_num), 
                       "sd" = sd_cv)
kableExtra::kable_styling(knitr::kable(cv_table))
```

From the boxplot graph, we could see as `k` increases, the cross-validation error's median decreases and width decreases.

Also, from the table, we could see as `k` increases, the cross-validation error's mean decreases and the standard deviation decreases.

Those two indicates the same thing -- as `k` increases, test is more accurate. This makes sense since the rule of thumb, `k = 5` and `k = 10` tend to result in an ideal balance in terms of the bias-variance tradeoff.
